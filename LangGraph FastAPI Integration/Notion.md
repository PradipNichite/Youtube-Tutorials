# Integrate LangGraph Rag Agent into FastAPI

### Commands

```bash
pip install uv
uv pip install -r requirements.txt
uvicorn main:app 

```

### Table Schema Details

Here is a more in-depth look at each table's structure.

### 1. `chat_history`

This table is designed to log every interaction between a user and the AI model, which is useful for tracking chat history, debugging, and analysis.

| Column | Data Type | Constraints & Notes | Description |
| --- | --- | --- | --- |
| **id** | `INTEGER` | `PRIMARY KEY`, `AUTOINCREMENT` | A unique, auto-incrementing identifier for each log entry. |
| **session_id** | `TEXT` |  | An identifier to group all messages belonging to a single conversation or session. |
| **user_query** | `TEXT` |  | The exact question or prompt submitted by the user. |
| **gpt_response** | `TEXT` |  | The corresponding response generated by the AI model. |
| **model** | `TEXT` |  | The specific AI model (e.g., 'gpt-4', 'claude-3') that was used. |
| **created_at** | `TIMESTAMP` | `DEFAULT CURRENT_TIMESTAMP` | The date and time when the log entry was created, set automatically. |

### 2. `document_store`

This table acts as a registry for documents that have been uploaded, likely to be used as a knowledge base for a Retrieval-Augmented Generation (RAG) system.

| Column | Data Type | Constraints & Notes | Description |
| --- | --- | --- | --- |
| **id** | `INTEGER` | `PRIMARY KEY`, `AUTOINCREMENT` | A unique, auto-incrementing identifier for each document record. |
| **filename** | `TEXT` |  | The name of the document file that was uploaded. |
| **upload_timestamp** | `TIMESTAMP` | `DEFAULT CURRENT_TIMESTAMP` | The date and time when the document was uploaded, set automatically. |

When a user asks a follow-up question, the retriever in a RAG workflow only receives the exact text we pass it—it doesn’t see the earlier turns.

So if the conversation goes:

1. **Q:** “What is the GreenGrow EcoHarvest System?”
    
    *We fetch docs that say it launched in 2021.*
    
2. **Q:** “When was it introduced?”

and we forward “When was it introduced?” to the retriever, it has no subject and returns irrelevant or empty results.

If we first rewrite the follow-up to the full form—“When was the GreenGrow EcoHarvest System introduced?”—the retriever can match “GreenGrow EcoHarvest System” in the document store, pull the correct paragraph, and the RAG pipeline answers accurately. Contextualising ensures every search query carries the full subject, so follow-ups work just as well as first questions.

![image.png](image.png)

```python
# Set up prompts and chains
contextualize_q_system_prompt = (
    "Given a chat history and the latest user question "
    "which might reference context in the chat history, "
    "formulate a standalone question which can be understood "
    "without the chat history. Do NOT answer the question, "
    "just reformulate it if needed and otherwise return it as is."
)

contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", contextualize_q_system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])
```

Lets analyse content from api folder. I just seperated my colab notebook into multiple python modules. My goal is to integrate langraph agent with chat endpoint.

Testing

```bash
"question": "Hi My name is Pradip",
"question": "Whats my name",
```

```bash
{
  "question": "GreenGrow EcoHarvest System when was introduced",
  "model": "gpt-4.1"
}
```